{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "# !pip install hazm\n",
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install tqdm\n",
    "# !pip3 install sklearn\n",
    "# !pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import codecs\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from hazm import *\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DIRNAME = '../new-dataset-with-category'\n",
    "data = []\n",
    "for i in range(1, 31):\n",
    "    with open(f'{DIRNAME}/namnak-{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "        data.extend(json.loads(f.read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7053"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 7053/7053 [00:00<00:00, 1149278.40it/s]\n"
     ]
    }
   ],
   "source": [
    "category_dictionary = {\n",
    "    'سلامت روان' : 0,\n",
    "    'دهان و دندان' : 1,\n",
    "    'پوست و مو' : 2,\n",
    "    'تغذیه' : 3,\n",
    "    'سلامت خانواده' : 4,\n",
    "    'سلامت جنسی' : 5,\n",
    "    'پیشگیری و بیماریها' : 6\n",
    "}\n",
    "\n",
    "doc_num = {_: 0 for _ in category_dictionary.values()}\n",
    "doc_num_threshold = 350\n",
    "\n",
    "selected_data = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(data))):\n",
    "    doc = data[i]\n",
    "    if doc['category'] in category_dictionary.keys() and doc_num[category_dictionary[doc['category']]] < doc_num_threshold:\n",
    "        doc_num[category_dictionary[doc['category']]] += 1\n",
    "        selected_data.append(doc)\n",
    "data = selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2434"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 2434/2434 [00:02<00:00, 976.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_data = []\n",
    "for item in tqdm.tqdm(data):\n",
    "    normalized_data.append({\"title\" : normalizer.normalize(item['title']), \"text\":normalizer.normalize(item['abstract'] + \" \" + \" \".join(item['paragraphs'])), \"link\": item['link']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ارتودنسی روشی برای زیبایی و مرتب کردن دندان‌ها می‌باشد اما این روش عوارض جانبی در داخل یا بیرون دهان ایجاد می‌کند که باید قبل از ارتودنسی بدانید. انواع عوارض ارتودنسی را بشناسید درمان ارتودنسی به اصلاح نامرتبی‌های دندان از طریق براکت‌های ثابت و یا متحرک می‌پردازد. جدا از اثر زیبایی می‌توان گفت که این درمان امکان آسیب رسانی و عوارض جانبی را نیز دارد که کاملا ناخواسته است. عوارض جانبی آن می‌تواند در داخل دهان یا بیرون دهان و یا کل بدن اتفاق بیافتد. اثرات جانبی در داخل دهان شامل موارد زیر است: اثرات روی دندان اثر روی بافت لثه اثرات روی بافت نرم اثرات روی دندان کلسیم و یا مواد آهکی روی سطح دندان باعث ایجاد لکه‌های سفید می‌شود و یکی از مشکلات رایج داخل دهانی حین ارتودنسی می‌باشد. این مواد آهکی روی سطح دندان به دلیل اسید تولید شده از پلاکت‌های دارای باکتری ایجاد می‌شود. پروسه تشکیل مواد آهکی روی دندان بسیار سریع اتفاق می‌افتد و اغلب یک ماه بعد از درمان ارتودنسی مخصوصا ارتودنسی‌های ثابت پدیدار می‌گردد. بنابراین بسیار مهم است که بهداشت دهانی را به طور عالی در پیش بگیریم. همچنین از مصرف مواد اس\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.sample(normalized_data, 10)[0]['text'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persian Stopwords\n",
    "# https://github.com/sobhe/hazm/blob/master/hazm/data/stopwords.dat\n",
    "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_freq(data, stopwords=[]):\n",
    "    tokens = []\n",
    "    for item in tqdm.tqdm(data):\n",
    "        tokens.extend([_ for _ in word_tokenize(item['text'])  if _ not in stopwords])\n",
    "    return pd.DataFrame(FreqDist(tokens).most_common(30)), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 2434/2434 [00:04<00:00, 543.38it/s]\n"
     ]
    }
   ],
   "source": [
    "freq_analysis, tokens = create_dataframe_freq(normalized_data, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>92860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>،</td>\n",
       "      <td>66676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:</td>\n",
       "      <td>12152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>استفاده</td>\n",
       "      <td>10567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>پوست</td>\n",
       "      <td>8431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>بدن</td>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>درمان</td>\n",
       "      <td>6926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>مصرف</td>\n",
       "      <td>6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>دندان</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>آب</td>\n",
       "      <td>6175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>کاهش</td>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>کمک</td>\n",
       "      <td>5728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>مواد</td>\n",
       "      <td>5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>انجام</td>\n",
       "      <td>5050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>صورت</td>\n",
       "      <td>4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>افزایش</td>\n",
       "      <td>4811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>دهید</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ایجاد</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>قرار</td>\n",
       "      <td>4238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>سلامت</td>\n",
       "      <td>4117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>بیماری</td>\n",
       "      <td>4079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>جنسی</td>\n",
       "      <td>3915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>)</td>\n",
       "      <td>3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>خون</td>\n",
       "      <td>3734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>درد</td>\n",
       "      <td>3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>کار</td>\n",
       "      <td>3630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>غذایی</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>روغن</td>\n",
       "      <td>3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>آن‌ها</td>\n",
       "      <td>3442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1\n",
       "0         .  92860\n",
       "1         ،  66676\n",
       "2         :  12152\n",
       "3   استفاده  10567\n",
       "4      پوست   8431\n",
       "5       بدن   7243\n",
       "6     درمان   6926\n",
       "7      مصرف   6892\n",
       "8     دندان   6692\n",
       "9        آب   6175\n",
       "10     کاهش   6039\n",
       "11      کمک   5728\n",
       "12     مواد   5575\n",
       "13    انجام   5050\n",
       "14     صورت   4965\n",
       "15   افزایش   4811\n",
       "16     دهید   4433\n",
       "17    ایجاد   4413\n",
       "18     قرار   4238\n",
       "19    سلامت   4117\n",
       "20   بیماری   4079\n",
       "21     جنسی   3915\n",
       "22        )   3738\n",
       "23      خون   3734\n",
       "24      درد   3633\n",
       "25      کار   3630\n",
       "26    غذایی   3624\n",
       "27        (   3571\n",
       "28     روغن   3454\n",
       "29    آن‌ها   3442"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [normalizer.normalize(x.strip()) for x in codecs.open('custom_stopwords.txt','r','utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stop_words = custom_stop_words + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 2434/2434 [00:04<00:00, 563.92it/s]\n"
     ]
    }
   ],
   "source": [
    "freq_analysis, tokens = create_dataframe_freq(normalized_data, total_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>استفاده</td>\n",
       "      <td>10567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>پوست</td>\n",
       "      <td>8431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بدن</td>\n",
       "      <td>7243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>درمان</td>\n",
       "      <td>6926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مصرف</td>\n",
       "      <td>6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>دندان</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>آب</td>\n",
       "      <td>6175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>کاهش</td>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>کمک</td>\n",
       "      <td>5728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>مواد</td>\n",
       "      <td>5575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>انجام</td>\n",
       "      <td>5050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>صورت</td>\n",
       "      <td>4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>افزایش</td>\n",
       "      <td>4811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>دهید</td>\n",
       "      <td>4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ایجاد</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>قرار</td>\n",
       "      <td>4238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>سلامت</td>\n",
       "      <td>4117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>بیماری</td>\n",
       "      <td>4079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>جنسی</td>\n",
       "      <td>3915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>خون</td>\n",
       "      <td>3734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>درد</td>\n",
       "      <td>3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>کار</td>\n",
       "      <td>3630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>غذایی</td>\n",
       "      <td>3624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>روغن</td>\n",
       "      <td>3454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>آن‌ها</td>\n",
       "      <td>3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>روز</td>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>دهان</td>\n",
       "      <td>3223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>دلیل</td>\n",
       "      <td>3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>می‌توانید</td>\n",
       "      <td>3113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>مشکلات</td>\n",
       "      <td>3009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1\n",
       "0     استفاده  10567\n",
       "1        پوست   8431\n",
       "2         بدن   7243\n",
       "3       درمان   6926\n",
       "4        مصرف   6892\n",
       "5       دندان   6692\n",
       "6          آب   6175\n",
       "7        کاهش   6039\n",
       "8         کمک   5728\n",
       "9        مواد   5575\n",
       "10      انجام   5050\n",
       "11       صورت   4965\n",
       "12     افزایش   4811\n",
       "13       دهید   4433\n",
       "14      ایجاد   4413\n",
       "15       قرار   4238\n",
       "16      سلامت   4117\n",
       "17     بیماری   4079\n",
       "18       جنسی   3915\n",
       "19        خون   3734\n",
       "20        درد   3633\n",
       "21        کار   3630\n",
       "22      غذایی   3624\n",
       "23       روغن   3454\n",
       "24      آن‌ها   3442\n",
       "25        روز   3299\n",
       "26       دهان   3223\n",
       "27       دلیل   3201\n",
       "28  می‌توانید   3113\n",
       "29     مشکلات   3009"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 2434/2434 [00:04<00:00, 565.27it/s]\n"
     ]
    }
   ],
   "source": [
    "all_doc_tokens = []\n",
    "for item in tqdm.tqdm(normalized_data):\n",
    "    all_doc_tokens.append([_ for _ in word_tokenize(item['text'])  if _ not in total_stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def get_lemma_set(tok, opt=1):\n",
    "    if opt ==1:\n",
    "        return stemmer.stem(tok)\n",
    "    if opt ==2:\n",
    "        return lemmatizer.lemmatize(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 2\n",
    "tokens_after_lem = []\n",
    "\n",
    "for tokens in all_doc_tokens:\n",
    "    tokens_after_lem.append([get_lemma_set(token, opt) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['آید', 'توان', 'تواند', 'توانند', 'رسد', 'رود', 'سال', 'نمی', 'های', 'گوید', 'گویند'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words=total_stop_words, ngram_range=(1,1))\n",
    "doc_term_mat = tfidfvectorizer.fit_transform([' '.join(doc) for doc in tokens_after_lem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tfidf vectorizer\n",
    "with open('../models/tfidfvectorizer-clustering.pk', 'wb') as fin:\n",
    "    pickle.dump(tfidfvectorizer, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tfidf vectorizer\n",
    "with open('../models/tfidfvectorizer-clustering.pk', 'rb') as fin:\n",
    "    tfidfvectorizer_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x25290 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvectorizer_loaded.transform(['ریزش مو در مردان'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2434, 25290)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrix\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "sparse.save_npz(\"../models/doc-term-tfidf-clustering.npz\", doc_term_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matrix\n",
    "doc_term_mat = sparse.load_npz(\"../models/doc-term-tfidf-clustering.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2434, 25290)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.104554854956746"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvectorizer.idf_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_آسم'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvectorizer.get_feature_names_out()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1632"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(tfidfvectorizer.get_feature_names_out() == 'آب')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat[4].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "from hazm import *\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "\n",
    "class TfidfEmbedingModel:\n",
    "    def __init__(self):\n",
    "        self.doc_term_mat = sparse.load_npz(\"../models/doc-term-tfidf-clustering.npz\")\n",
    "        with open('../models/tfidfvectorizer-clustering.pk', 'rb') as fin:\n",
    "            self.tfidfvectorizer = pickle.load(fin)\n",
    "        self.normalizer = Normalizer()\n",
    "        stopwords = [self.normalizer.normalize(x.strip()) for x in codecs.open('../stopwords/stopwords.txt','r','utf-8').readlines()]\n",
    "        custom_stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('../stopwords/custom_stopwords.txt','r','utf-8').readlines()]\n",
    "        self.total_stop_words = custom_stop_words + stopwords\n",
    "        \n",
    "    def get_doc_embeding(self, doc, index):\n",
    "        return ((self.get_text_embeding(index) * 19) + self.get_title_embeding(doc['title'])) / 20\n",
    "    \n",
    "    def get_text_embeding(self, index):\n",
    "        return self.doc_term_mat[index].toarray()[0]\n",
    "            \n",
    "    def get_title_embeding(self, title):\n",
    "        tokens = [_ for _ in word_tokenize(self.normalizer.normalize(title)) if _ not in self.total_stop_words]\n",
    "        emb = self.tfidfvectorizer.transform([' '.join(tokens)])\n",
    "        return emb[0].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_embeding_model = TfidfEmbedingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2434/2434 [00:00<00:00, 2564.31it/s]\n"
     ]
    }
   ],
   "source": [
    "category_dictionary = {\n",
    "    'سلامت روان' : 0,\n",
    "    'دهان و دندان' : 1,\n",
    "    'پوست و مو' : 2,\n",
    "    'تغذیه' : 3,\n",
    "    'سلامت خانواده' : 4,\n",
    "    'سلامت جنسی' : 5,\n",
    "    'پیشگیری و بیماریها' : 6\n",
    "}\n",
    "\n",
    "doc_num = {_: 0 for _ in category_dictionary.values()}\n",
    "doc_num_threshold = 350\n",
    "\n",
    "X_train = list()\n",
    "Y_test = list()\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(data))):\n",
    "    doc = data[i]\n",
    "    if doc['category'] in category_dictionary.keys() and doc_num[category_dictionary[doc['category']]] < doc_num_threshold:\n",
    "        doc_num[category_dictionary[doc['category']]] += 1\n",
    "        Y_test.append(category_dictionary[doc['category']])\n",
    "        X_train.append(tfidf_embeding_model.get_doc_embeding(doc, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0 : 483\n",
      "cluster1 : 121\n",
      "cluster2 : 283\n",
      "cluster3 : 517\n",
      "cluster4 : 212\n",
      "cluster5 : 89\n",
      "cluster6 : 729\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=7, n_init=50, max_iter=1000, tol=1e-8).fit(X_train)\n",
    "Y_predicted = kmeans.predict(X_train)\n",
    "Y_center = [kmeans.cluster_centers_[y] for y in Y_predicted]\n",
    "for i in range(0,7):\n",
    "    print(f'cluster{i} : {Y_predicted.tolist().count(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS = 1981.7385132457648\n"
     ]
    }
   ],
   "source": [
    "print(f'RSS = {kmeans.inertia_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contin_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contin_matrix, axis=0)) / np.sum(contin_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6261298274445357"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_score(Y_test, Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03852207731586154"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.silhouette_score(X_train, Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.72715910762129"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davies_bouldin_score(X_train, Y_predicted)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f336af7d54ba0f0c1daaf2256eb85f31e983e88153daf7a27ef3ea6c724faba4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
