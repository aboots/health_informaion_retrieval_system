{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip3 -q install datasets\n",
    "# !pip3 -q install transformers\n",
    "# !pip3 -q install sentencepiece\n",
    "# !pip3 -q install hazm\n",
    "# !pip3 -q install clean-text[gpl]\n",
    "# !pip3 install faiss-cpu\n",
    "# !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import codecs\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from hazm import *\n",
    "import torch\n",
    "from transformers import BigBirdModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model for first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5192666a544d1ba52dd035520a77be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=314339179, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdModel were not initialized from the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj and are newly initialized: ['bert.pooler.weight', 'bert.pooler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj were not used when initializing BigBirdModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdModel were not initialized from the model checkpoint at SajjadAyoubi/distil-bigbird-fa-zwnj and are newly initialized: ['bert.pooler.weight', 'bert.pooler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4511d7254c47e986e396e7660615eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=365, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf07d61babc456c912743511134957b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=426422, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7ddd87f8e94d06bc3ae5f26974e564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=112, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"SajjadAyoubi/distil-bigbird-fa-zwnj\"\n",
    "model = BigBirdModel.from_pretrained(MODEL_NAME, block_size=32)\n",
    "model = BigBirdModel.from_pretrained(MODEL_NAME, attention_type=\"original_full\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/pretrained-transformer-tokenizer\\\\tokenizer_config.json',\n",
       " '../models/pretrained-transformer-tokenizer\\\\special_tokens_map.json',\n",
       " '../models/pretrained-transformer-tokenizer\\\\vocab.txt',\n",
       " '../models/pretrained-transformer-tokenizer\\\\added_tokens.json',\n",
       " '../models/pretrained-transformer-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../models/pretrained-transformer-model.model')\n",
    "\n",
    "tokenizer.save_pretrained('../models/pretrained-transformer-tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigBirdModel.from_pretrained('../models/pretrained-transformer-model.model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/pretrained-transformer-tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize docs type1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join paragraphs up to 300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "source_names = ['hidoctor', 'namnak']\n",
    "threshold = 300\n",
    "\n",
    "for source in source_names:\n",
    "    for i in range(1, 8):\n",
    "        data = []\n",
    "        with open(f'../old-dataset/{source}-{i}.json', 'r', encoding=\"utf-8\") as j:\n",
    "            contents = json.loads(j.read())\n",
    "            for cont in contents:\n",
    "                out = []\n",
    "                if 'abstract' in cont.keys():\n",
    "                    out.append(cont['abstract'])\n",
    "                if len(cont['paragraphs']):\n",
    "                    current = cont['paragraphs'][0]\n",
    "                    current_len = len(current.split())\n",
    "                    for chunk in cont['paragraphs'][1:]:\n",
    "                        chunk_len = len(chunk.split())\n",
    "                        if current_len + chunk_len < threshold:\n",
    "                            current += ' ' + chunk\n",
    "                            current_len += chunk_len\n",
    "                        else:\n",
    "                            if len(current) > 10: \n",
    "                                out.append(current)\n",
    "                                current = chunk\n",
    "                                current_len = chunk_len\n",
    "                            else:\n",
    "                                out.append(current + ' '.join(chunk.split()[0:threshold]))\n",
    "                                current = ' '.join(chunk.split()[threshold : min(chunk_len, threshold + 299)])\n",
    "                                current_len = len(current.split())\n",
    "                if len(current) > 3:\n",
    "                    out.append(current)\n",
    "\n",
    "                item = {'title' : cont['title'], 'paragraphs': out}\n",
    "                data.append(item)\n",
    "        os.makedirs('data-edited', exist_ok=True)\n",
    "        with open(f'data-edited/{source}-p{i}.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = 'data-edited'\n",
    "data = []\n",
    "for i in range(1,8):\n",
    "    with open(f'{DIRNAME}/namnak-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "for i in range(1,8):\n",
    "    with open(f'{DIRNAME}/hidoctor-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "with open(f'data-edited/data-all-new.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data-edited/data-all-new.json', 'r', encoding=\"utf-8\") as f:\n",
    "       data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4322/4322 [00:06<00:00, 656.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_data = []\n",
    "for item in tqdm.tqdm(data):\n",
    "    normalized_data.append({\"title\" : item['title'],\n",
    "                            \"paragraphs\":[normalizer.normalize(paragraph) for paragraph in item['paragraphs']]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "custom_stop_words = [normalizer.normalize(x.strip()) for x in codecs.open('custom_stopwords.txt','r','utf-8').readlines()]\n",
    "total_stop_words = custom_stop_words + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_stopwords = []\n",
    "for item in normalized_data:\n",
    "    data_without_stopwords.append({\"title\" : item['title'],\n",
    "                            \"paragraphs\":[' '.join([_ for _ in word_tokenize(paragraph)  if _ not in total_stop_words])\n",
    "                                          for paragraph in item['paragraphs']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "i = 0\n",
    "for document in data_without_stopwords:\n",
    "    title_encoded = model(**tokenizer(document['title'], return_tensors='pt'))[0].detach().squeeze()\n",
    "    title_encoded = torch.mean(title_encoded, dim=0)\n",
    "\n",
    "    encoded_paragraphs = [model(**tokenizer(doc, return_tensors='pt'))[0].detach().squeeze() \n",
    "                          for doc in document['paragraphs']]\n",
    "    averaged_vectors = [torch.mean(vector, dim=0) for vector in encoded_paragraphs]\n",
    "    paragraphs_averaged = torch.stack(averaged_vectors).mean(dim=0)\n",
    "    \n",
    "    vectors.append({'index': i, 'title': document['title'],\n",
    "                    'vector' : [title_encoded.numpy().tolist(), paragraphs_averaged.numpy().tolist()]})\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/transformer_vectors-new.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorize docs type2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join paragraphs up to 300 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '../dataset'\n",
    "threshold = 300\n",
    "\n",
    "data = []\n",
    "for i in range(1, 8):\n",
    "    with open(f'{DIRNAME}/namnak-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "for i in range(1, 8):\n",
    "    with open(f'{DIRNAME}/hidoctor-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "\n",
    "            \n",
    "data_paragraphed = []\n",
    "link_data = {}\n",
    "for i, doc in enumerate(data):\n",
    "    out = []\n",
    "    current =  ''\n",
    "    current_len = 0\n",
    "    for sent in doc['text'].split('. '):\n",
    "        if current_len + len(sent.split()) < threshold:\n",
    "            current += ' ' + sent\n",
    "            current_len += len(sent.split())\n",
    "        else:\n",
    "            out.append(current)\n",
    "            current = sent\n",
    "            current_len = len(sent.split())\n",
    "    if len(current) > 10:\n",
    "        out.append(current)\n",
    "    item = {'title': doc['title'], 'paragraphs': out}\n",
    "    data_paragraphed.append(item)\n",
    "    link_data[i] = doc['link']\n",
    "with open(f'data-edited/data-all2.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(data_paragraphed, f)\n",
    "with open(f'../models/transformers-link-docs-data.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(link_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data-edited/data-all2.json', 'r', encoding=\"utf-8\") as f:\n",
    "       data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4322/4322 [00:06<00:00, 627.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_data = []\n",
    "for item in tqdm.tqdm(data):\n",
    "    normalized_data.append({\"title\" : item['title'],\n",
    "                            \"paragraphs\":[normalizer.normalize(paragraph) for paragraph in item['paragraphs']]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "custom_stop_words = [normalizer.normalize(x.strip()) for x in codecs.open('custom_stopwords.txt','r','utf-8').readlines()]\n",
    "total_stop_words = custom_stop_words + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_stopwords = []\n",
    "for item in normalized_data:\n",
    "    data_without_stopwords.append({\"title\" : item['title'],\n",
    "                            \"paragraphs\":[' '.join([_ for _ in word_tokenize(paragraph)  if _ not in total_stop_words])\n",
    "                                          for paragraph in item['paragraphs']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors1 = []\n",
    "\n",
    "i = 0\n",
    "for document in data_without_stopwords:\n",
    "    title_encoded = model(**tokenizer(document['title'], return_tensors='pt'))[0].detach().squeeze()\n",
    "    title_encoded = torch.mean(title_encoded, dim=0)\n",
    "\n",
    "    encoded_paragraphs = [model(**tokenizer(doc, return_tensors='pt'))[0].detach().squeeze() \n",
    "                          for doc in document['paragraphs']]\n",
    "    averaged_vectors = [torch.mean(vector, dim=0) for vector in encoded_paragraphs]\n",
    "    paragraphs_averaged = torch.stack(averaged_vectors).mean(dim=0)\n",
    "    \n",
    "    vectors1.append({'index': i, 'title': document['title'],\n",
    "                    'vector' : [title_encoded.numpy().tolist(), paragraphs_averaged.numpy().tolist()]})\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/transformer_vectors-new1.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(vectors1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mix to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '../dataset'\n",
    "data = []\n",
    "for i in range(1, 8):\n",
    "    with open(f'{DIRNAME}/namnak-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "for i in range(1, 8):\n",
    "    with open(f'{DIRNAME}/hidoctor-p{i}.json', 'r', encoding=\"utf-8\") as f:\n",
    "           data += json.loads(f.read())\n",
    "\n",
    "useless_docs = []\n",
    "for i in range(len(data)):\n",
    "    if len(data[i]['text']) < 100:\n",
    "        useless_docs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/transformer_vectors-new.json', 'r', encoding=\"utf-8\") as f:\n",
    "    emb = json.loads(f.read())\n",
    "with open('../models/transformer_vectors-new1.json', 'r', encoding=\"utf-8\") as f:\n",
    "    emb1 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emb = []\n",
    "for i in range(0, len(emb)):\n",
    "    item = {'index' : emb[i]['index'],'title' : emb[i]['title'],\n",
    "            'vector': ((((np.array(emb[i]['vector'][0])*1 + np.array(emb[i]['vector'][1])*9)/10)*3.5 + \n",
    "                       ((np.array(emb1[i]['vector'][0])*1 + np.array(emb1[i]['vector'][1])*9)/10)*0.5)/4).tolist()}\n",
    "    new_emb.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/transformer_vectors-new-mixed.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(new_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in new_emb:\n",
    "    if doc['index'] in useless_docs:\n",
    "        new_emb.remove(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/transformer_vectors-new-mixed.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(new_emb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BigBirdModel, AutoTokenizer\n",
    "\n",
    "class TransformerEmb:\n",
    "    def __init__(self):\n",
    "        self.model = BigBirdModel.from_pretrained('../models/pretrained-transformer-model.model')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('../models/pretrained-transformer-tokenizer')\n",
    "        with open('../models/transformers-link-docs-data.json', 'r', encoding=\"utf-8\") as f:\n",
    "            self.docs_links = json.loads(f.read())\n",
    "        with open('../models/transformer_vectors-new-mixed.json', 'r', encoding=\"utf-8\") as f:\n",
    "            self.docs_embs = json.loads(f.read())\n",
    "        self.normalizer = Normalizer()\n",
    "        stopwords = [self.normalizer.normalize(x.strip()) for x in codecs.open('stopwords.txt','r','utf-8').readlines()]\n",
    "        custom_stop_words = [self.normalizer.normalize(x.strip()) for x in codecs.open('custom_stopwords.txt','r','utf-8').readlines()]\n",
    "        self.total_stop_words = custom_stop_words + stopwords\n",
    "    \n",
    "    def print_similars(self, query, k=10):\n",
    "        ls = self.get_query(query, k)\n",
    "        for i, item in enumerate(ls):\n",
    "            print(f'{i + 1}- title: {item[0]}')\n",
    "            print(f'{i + 1}- link: {item[1]}')\n",
    "            print('-------------------------')\n",
    "    \n",
    "    def get_query(self, query, k=10):\n",
    "        encoded_query = self.model(**self.tokenizer(query, return_tensors='pt'))[0].detach().squeeze()\n",
    "        encoded_query = torch.mean(encoded_query, dim=0).numpy()\n",
    "        return self.nearest_neighbor(encoded_query, self.docs_embs, k)\n",
    "    \n",
    "    def cosine_similarity(self, vector_1: np.ndarray, vector_2: np.ndarray) -> float:\n",
    "        return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1) *\n",
    "                                          np.linalg.norm(vector_2))\n",
    "    \n",
    "    def nearest_neighbor(self, v, doc_embs, k):\n",
    "        data = {}\n",
    "        for doc in doc_embs:\n",
    "            data[doc['title']] = (self.cosine_similarity(v, np.array(doc['vector'])), self.docs_links[str(doc['index'])])\n",
    "        return [(k,v[1]) for k, v in sorted(data.items(), key=lambda item: item[1][0])][::-1][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerEmb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1- title: روش جدید برای درمان ریزش مو و کچلی\n",
      "1- link: https://namnak.com/درمان-کچلی.p31230\n",
      "-------------------------\n",
      "2- title: دلایل ریزش مو در زنان\n",
      "2- link: https://namnak.com/ریزش-مو.p65282\n",
      "-------------------------\n",
      "3- title: شایعترین عوامل موثر در قد کودکان\n",
      "3- link: https://namnak.com/قد-کودکان.p60301\n",
      "-------------------------\n",
      "4- title: چرا نگهداری دندان شیری مهم است و فواید ان چیست؟\n",
      "4- link: https://namnak.com/نگهداری-دندان-شیری.p58074\n",
      "-------------------------\n",
      "5- title: 5 علامت نگران کننده در مورد سلامت بدن\n",
      "5- link: https://namnak.com/health-problems-symptoms.p59240\n",
      "-------------------------\n",
      "6- title: وقتی موهای بدن هشدار می دهند !\n",
      "6- link: https://namnak.com/body-hair-says-about-health.p77001\n",
      "-------------------------\n",
      "7- title: چهار تا از بهترین شامپو ها برای جلوگیری از ریزش مو\n",
      "7- link: https://www.hidoctor.ir/354908_%da%86%d9%87%d8%a7%d8%b1-%d8%aa%d8%a7-%d8%a7%d8%b2-%d8%a8%d9%87%d8%aa%d8%b1%db%8c%d9%86-%d8%b4%d8%a7%d9%85%d9%be%d9%88-%d9%87%d8%a7-%d8%a8%d8%b1%d8%a7%db%8c-%d8%ac%d9%84%d9%88%da%af%db%8c%d8%b1%db%8c.html/\n",
      "-------------------------\n",
      "8- title: روش ساده درمان ریزش مو در طول شیمی درمانی\n",
      "8- link: https://namnak.com/ریزش-مو-و-شیمی-درمانی.p55334\n",
      "-------------------------\n",
      "9- title: چگونه ریزش مو را درمان کنیم؟\n",
      "9- link: https://www.hidoctor.ir/355075_%da%86%da%af%d9%88%d9%86%d9%87-%d8%b1%db%8c%d8%b2%d8%b4-%d9%85%d9%88-%d8%b1%d8%a7-%d8%af%d8%b1%d9%85%d8%a7%d9%86-%da%a9%d9%86%db%8c%d9%85%d8%9f.html/\n",
      "-------------------------\n",
      "10- title: با کدام روغن های گیاهی میتوان از ریزش مو جلوگیری کرد؟\n",
      "10- link: https://namnak.com/جلوگیری-از-ریزش-مو.p49871\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "transformer.print_similars('ریزش مو')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
